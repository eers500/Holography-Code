{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-90c1db9f1a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileopenbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/media/erick/NuevoVol/LINUX_LAP/PhD/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# DF = pd.read_csv('/home/erick/Documents/PhD/Colloids/20x_50Hz_100us_642nm_colloids_2000frames_2000frames_rayleighSommerfeld_Results.csv', index_col=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid file path or buffer object type: {type(filepath_or_buffer)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import mpld3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "mpl.rc('figure',  figsize=(10, 6))\n",
    "import functions as f\n",
    "import sklearn.cluster as cl\n",
    "import time\n",
    "import easygui as gui\n",
    "import hdbscan\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from hungarian_algorithm import algorithm\n",
    "\n",
    "PATH = gui.fileopenbox(default='/media/erick/NuevoVol/LINUX_LAP/PhD/')\n",
    "DF = pd.read_csv(PATH, index_col=0)\n",
    "\n",
    "# DF = pd.read_csv('/home/erick/Documents/PhD/Colloids/20x_50Hz_100us_642nm_colloids_2000frames_2000frames_rayleighSommerfeld_Results.csv', index_col=0)\n",
    "# DF= pd.read_csv('/home/erick/Documents/PhD/Colloids/20x_50Hz_100us_642nm_colloids_2000frames_2000frames_modified_propagator_Results.csv', index_col=0)\n",
    "# DF = pd.read_csv('/media/erick/NuevoVol/LINUX_LAP/PhD/Pseudomonas/2017-10-23/red_laser_100fps_200x_0_135msec_1_500_FRAMES_MODIFIED.csv', index_col=0)\n",
    "# DF = pd.read_csv('/media/erick/NuevoVol/LINUX_LAP/PhD/Pseudomonas/2017-10-23/red_laser_100fps_200x_0_135msec_1/red_laser_100fps_200x_0_135msec_1_2001_FRAMES_MODIFIED.csv', index_col=0)\n",
    "# DF = pd.read_csv('/media/erick/NuevoVol/LINUX_LAP/PhD/Pseudomonas/2017-10-23/red_laser_100fps_200x_0_135msec_1/red_laser_100fps_200x_0_135msec_1_2001_FRAMES_RS_TH019.csv', index_col=0)\n",
    "\n",
    "DF.index = np.arange(len(DF))\n",
    "# D = DF[['X', 'Y', 'Z', 'FRAME']].values\n",
    "D = DF.values\n",
    "# DD = A[:, :3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect tracks using ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KMeans, DBSCAN or HA\n",
    "# method = 'DBSCAN'\n",
    "method = gui.choicebox(msg='Choose a tracking Algorithm', title='Choose', choices=['KMeans', 'DBSCAN', 'HA'])\n",
    "\n",
    "\n",
    "if method == 'KMeans':\n",
    "\n",
    "    #% K-Means for track detection\n",
    "    T0_kmeans = time.time()\n",
    "    kmeans = cl.KMeans(n_clusters=D[D[:, 5] == 0].shape[0], init='k-means++').fit(DF[['X', 'Y', 'Z']])\n",
    "    DF['PARTICLE'] = kmeans.labels_\n",
    "    LINKED = DF\n",
    "    T_kmeans = time.time() -  T0_kmeans\n",
    "\n",
    "elif method == 'DBSCAN':\n",
    "\n",
    "    #% Density Based Spatial Clustering (DBSC)\n",
    "    T0_DBSCAN = time.time()\n",
    "    DBSCAN = cl.DBSCAN(eps=5, min_samples=8).fit(DF[['X', 'Y', 'Z']])\n",
    "    DF['PARTICLE'] = DBSCAN.labels_\n",
    "    LINKED = DF\n",
    "    L = LINKED.drop(np.where(LINKED.PARTICLE.values == -1)[0])\n",
    "    T_DBSCAN = time.time() - T0_DBSCAN\n",
    "\n",
    "elif method == 'HA':\n",
    "\n",
    "    #% Hungrian algorithm\n",
    "    T0_HA = time.time()\n",
    "    FRAME_DATA = np.empty(int(DF['FRAME'].max()), dtype=object)\n",
    "    SIZE = np.empty(FRAME_DATA.shape[0])\n",
    "    for i in range(FRAME_DATA.shape[0]):\n",
    "        FRAME_DATA[i] = D[D[:, 5] == i]\n",
    "        SIZE[i] = FRAME_DATA[i].shape[0]\n",
    "        \n",
    "    # To make a square cost matrix\n",
    "    # SIZES = np.empty(FRAME_DATA.shape[0])\n",
    "    # for i in range(FRAME_DATA.shape[0]):\n",
    "    #     if SIZE.max() != FRAME_DATA[i].shape[0]:\n",
    "    #         DIFF = int(SIZE.max() - FRAME_DATA[i].shape[0])\n",
    "    #         FRAME_DATA[i] = np.pad(FRAME_DATA[i], ((0, DIFF), (0, 0)), 'constant', constant_values=0)\n",
    "    #     SIZES[i] = FRAME_DATA[i].shape[0]\n",
    "            \n",
    "    \n",
    "    # particle_index = np.empty((int(SIZES[0]), len(FRAME_DATA)), dtype='int')\n",
    "    TRACKS = [FRAME_DATA[0]]\n",
    "    sizes = np.empty(len(FRAME_DATA), dtype='int')\n",
    "    sizes[0] = len(FRAME_DATA[0])\n",
    "    for k in range(len(FRAME_DATA)-1):  \n",
    "        print(k)\n",
    "        \n",
    "        if k==0:    \n",
    "            R1 = FRAME_DATA[k]\n",
    "            R2 = FRAME_DATA[k+1]\n",
    "        else:\n",
    "            R1 = TRACKS[k]\n",
    "            R2 = FRAME_DATA[k+1]\n",
    "        \n",
    "        # COST = np.empty((int(SIZE.max()), int(SIZE.max())))\n",
    "        COST = np.empty((len(R1), len(R2)))\n",
    "        # for i in range(int(SIZE.max())):\n",
    "        #     for j in range(int(SIZE.max())):        \n",
    "                \n",
    "        for i in range(len(R1)):\n",
    "            for j in range(len(R2)):\n",
    "                \n",
    "                COST[i, j] = np.sqrt(sum((R1[i, :3]- R2[j, :3])**2))      \n",
    "        # row_ind, particle_index[:, k] = linear_sum_assignment(COST)\n",
    "        row_ind, particle_index = linear_sum_assignment(COST)\n",
    "        sizes[k+1] = len(particle_index)\n",
    "        # print(len(particle_index))\n",
    "        # TRACKS.append(R2[particle_index[:, k], :])\n",
    "        TRACKS.append(R2[particle_index, :])\n",
    "    \n",
    "    # print(time.time() - T0)\n",
    "    \n",
    "    #%\n",
    "    TRACK = np.vstack(TRACKS)\n",
    "    TR = pd.DataFrame(TRACK, columns=['X', 'Y', 'Z', 'I_FS', 'I_GS', 'FRAME'])\n",
    "    # TR['PARTICLE'] = np.tile(np.arange(SIZES[0]), len(TRACKS))\n",
    "    \n",
    "    particle_column = []\n",
    "    for i in range(len(sizes)):\n",
    "        particle_column.append(np.arange(sizes[i]))    \n",
    "    \n",
    "    particle_column = np.concatenate(particle_column, axis=0)\n",
    "    TR['PARTICLE'] = particle_column\n",
    "    LINKED = TR\n",
    "    T_HA = time.time() - T0_HA\n",
    "    print(T_HA)\n",
    "\n",
    "LINKED = LINKED[LINKED.PARTICLE != -1]\n",
    "# ID = np.where(SHAPES >= 1000)[0]\n",
    "# IND  = np.in1d(TR.PARTICLE.values,  ID, invert=True)\n",
    "# LINKED = TR.loc[IND, :]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cubic Spline Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LINKED = LINKED[LINKED.PARTICLE != -1]\n",
    "\n",
    "# For Archea data that looks messy\n",
    "# value_counts = LINKED.PARTICLE.value_counts()\n",
    "# values = value_counts[value_counts.values < 1000]\n",
    "# idx = LINKED.PARTICLE.isin(values.index)\n",
    "# LINKED = LINKED.iloc[idx.values]\n",
    "\n",
    "spline_degree = 3  # 3 for cubic spline\n",
    "particle_num = np.sort(LINKED.PARTICLE.unique())\n",
    "T0_smooth = time.time()\n",
    "smoothed_curves = -np.ones((1, 4))\n",
    "for pn in particle_num:\n",
    "    # Do not use this\n",
    "    # L = LINKED[LINKED.PARTICLE == pn].values\n",
    "    # X = f.smooth_curve(L, spline_degree=spline_degree, lim=20, sc=3000)\n",
    "    \n",
    "    L = LINKED[LINKED.PARTICLE == pn]\n",
    "    if len(L) < 100:\n",
    "        continue\n",
    "    X = f.csaps_smoothing(L, smoothing_condition=0.999999, smooth_data=True)\n",
    "    \n",
    "    if X != -1:\n",
    "        smoothed_curves = np.vstack((smoothed_curves, np.stack((X[0], X[1], X[2], pn*np.ones_like(X[1])), axis=1))) \n",
    "\n",
    "smoothed_curves = smoothed_curves[1:, :]\n",
    "smoothed_curves_df = pd.DataFrame(smoothed_curves, columns=['X', 'Y' ,'Z', 'PARTICLE'])\n",
    "T_smooth = time.time() - T0_smooth\n",
    "\n",
    "# smoothed_curves_df.to_csv(PATH[:-4]+'_DBSCAN_smooth_200.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDP algorithm for each dimension separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% RDP algorithm for each dimension separately.\n",
    "%matplotlib notebook\n",
    "from rdp import rdp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# L = LINKED[LINKED.PARTICLE == 6]\n",
    "L = smoothed_curves_df[smoothed_curves_df.PARTICLE == 4]\n",
    "xx, yy, zz = L.X.values, L.Y.values, L.Z.values\n",
    "xx = xx.reshape(len(xx), 1)\n",
    "yy = yy.reshape(len(yy), 1)\n",
    "zz = zz.reshape(len(zz), 1)\n",
    "\n",
    "t = np.linspace(0, 1, len(xx))\n",
    "t = t.reshape(len(t), 1)\n",
    "\n",
    "stack =  np.hstack((xx, yy, zz))\n",
    "\n",
    "eps = 0.05\n",
    "# eps3d = 0.1\n",
    "# xyz = rdp(stack, epsilon=eps3d)\n",
    "x = rdp(np.hstack((t, xx)), epsilon=eps)\n",
    "y = rdp(np.hstack((t, yy)), epsilon=eps)\n",
    "z = rdp(np.hstack((t, zz)), epsilon=eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "XYZ = np.sort(np.concatenate((x[:, 0], y[:, 0], z[:, 0]), axis=0))\n",
    "unique = np.unique(XYZ, axis=0)\n",
    "unique_id, _ = np.where(t == unique)\n",
    "turns = np.concatenate((xx[unique_id], yy[unique_id], zz[unique_id]), axis=1)\n",
    "\n",
    "# fig = plt.figure(figsize=(7, 4.5))\n",
    "# ax0 = plt.subplot2grid((6,6), (0, 0), 2, 2)\n",
    "# ax0.plot(t, xx, 'b-')\n",
    "# ax0.plot(x[:, 0], x[:, 1], 'r.')\n",
    "# ax0.set_ylabel('x')\n",
    "# ax0.set_title('eps = '+np.str(eps))\n",
    "# ax0.grid()\n",
    "\n",
    "# ax1 = plt.subplot2grid((6, 6), (2, 0), 2, 2)\n",
    "# ax1.plot(t, yy, 'b-')\n",
    "# ax1.plot(y[:, 0], y[:, 1], 'r.')\n",
    "# ax1.set_ylabel('y')\n",
    "# ax1.grid()\n",
    "\n",
    "# ax2 = plt.subplot2grid((6, 6), (4, 0), 2, 2)\n",
    "# ax2.plot(t, zz, 'b-')\n",
    "# ax2.plot(z[:, 0], z[:, 1], 'r.')\n",
    "# ax2.set_ylabel('z')\n",
    "# ax2.grid()\n",
    "\n",
    "# #fig = plt.figure(figsize=(14, 9))\n",
    "# # ax3 = fig.add_subplot(111, projection='3d')\n",
    "# ax3 = plt.subplot2grid((6, 6), (0, 2), 6, 6, projection='3d')\n",
    "# ax3.plot(L.X, L.Y, L.Z, 'b-', label='original data')\n",
    "# ax3.plot(turns[:, 0], turns[:, 1], turns[:, 2], 'r.', label='Turns')\n",
    "# ax3.set_title('Merged Turns')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bokeh plots\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook, push_notebook\n",
    "from bokeh.layouts import row, column, gridplot\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "TOOLS = \"crosshair,pan,wheel_zoom,box_zoom,reset,box_select,lasso_select,hover\"\n",
    "p1 = figure(title='eps = '+np.str(eps), y_axis_label='x', \n",
    "            tools=TOOLS)\n",
    "p1.line(t[:, 0], xx[:, 0], legend_label=\"Temp.\", line_width=2)\n",
    "p1.circle(x[:, 0], x[:, 1], fill_color='red', line_color='red', radius=0.005)\n",
    "\n",
    "p2 = figure(title='eps = '+np.str(eps), y_axis_label='y', \n",
    "            tools=TOOLS)\n",
    "p2.line(t[:, 0], yy[:, 0], legend_label=\"Temp.\", line_width=2)\n",
    "p2.circle(y[:, 0], y[:, 1], fill_color='red', line_color='red', radius=0.005)\n",
    "\n",
    "p3 = figure(title='eps = '+np.str(eps), y_axis_label='z', \n",
    "            tools=TOOLS)\n",
    "p3.line(t[:, 0], zz[:, 0], legend_label=\"Temp.\", line_width=2)\n",
    "p3.circle(z[:, 0], z[:, 1], fill_color='red', line_color='red', radius=0.005)\n",
    "\n",
    "grid = gridplot([p1, p2, p3], ncols=1, plot_width=900, plot_height=150)\n",
    "show(grid)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 4.5))\n",
    "ax3 = fig.add_subplot(111, projection='3d')\n",
    "# ax3 = plt.subplot2grid((6, 6), (0, 0), 6, 6, projection='3d')\n",
    "ax3.plot(L.X, L.Y, L.Z, 'b-', label='original data')\n",
    "ax3.plot(turns[:, 0], turns[:, 1], turns[:, 2], 'r.', label='Turns')\n",
    "ax3.set_title('Merged Turns')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDP algorithm for 3D trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% RDP algorithm for 3D trajectoy simulataneously\n",
    "eps3d = 0.5\n",
    "xyz = rdp(stack, epsilon=eps3d)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 4.5))\n",
    "ax0 = plt.subplot2grid((6, 6), (0, 0), 6, 3, projection='3d')\n",
    "ax0.set_facecolor('none')\n",
    "ax0.plot(L.X, L.Y, L.Z, 'b-', label='original data')\n",
    "ax0.plot(xyz[:, 0], xyz[:, 1], xyz[:, 2], 'r.', label='Turns')\n",
    "ax0.set_title('Smoothed track')\n",
    "\n",
    "ax1 = plt.subplot2grid((6, 6), (0, 3), 6, 3, projection='3d')\n",
    "ax1.set_facecolor('none')\n",
    "ax1.plot(xyz[:, 0], xyz[:, 1], xyz[:, 2], 'b-', label='RDP reconstructed eps = '+np.str(eps3d))\n",
    "ax1.plot(xyz[:, 0], xyz[:, 1], xyz[:, 2], 'r.', label='Turns')\n",
    "ax1.set_title('RDP Simplification')\n",
    "ax1.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use derivatives of r (distance to origin) to sense changes in direction\n",
    "from scipy.signal import find_peaks\n",
    "r = np.sqrt(L.X.values**2 + L.Y.values**2 + L.Z.values**2)\n",
    "diff = np.diff(r)\n",
    "diff2 = np.diff(diff)\n",
    "peaks, _ = find_peaks(diff, height=0.04)\n",
    "peaks2, _ = find_peaks(diff2, height=np.min(diff2)-1)   # to get all local maxima\n",
    "\n",
    "fig = plt.figure(figsize=(9, 4.5))\n",
    "ax = fig.add_subplot(311)\n",
    "ax.plot(r)\n",
    "\n",
    "axx = fig.add_subplot(312)\n",
    "axx.plot(np.arange(len(diff)), diff)\n",
    "axx.plot(peaks, diff[peaks], '.')\n",
    "axx = fig.add_subplot(313)\n",
    "axx.plot(np.arange(len(diff2)), diff2)\n",
    "axx.plot(peaks2, diff2[peaks2], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 7))\n",
    "axxx = fig.add_subplot(121, projection='3d')\n",
    "axxx.plot(L.X, L.Y, L.Z, '-', label='original data')\n",
    "axxx.plot(L.X.values[peaks], L.Y.values[peaks], L.Z.values[peaks], 'r.')\n",
    "\n",
    "axxx = fig.add_subplot(122, projection='3d')\n",
    "axxx.plot(L.X, L.Y, L.Z, '-', label='original data')\n",
    "axxx.plot(L.X.values[peaks2], L.Y.values[peaks2], L.Z.values[peaks2], 'r.')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Scatter Plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "#%matplotlib qt\n",
    "\n",
    "p_number = 9\n",
    "CURVE_1 = LINKED[LINKED.PARTICLE == p_number]\n",
    "CURVE_2 = smoothed_curves_df[smoothed_curves_df.PARTICLE == p_number]\n",
    "\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(CURVE_1.X, CURVE_1.Y, CURVE_1.Z, 'r.', label='Detected Positions', c=np.arange(len(CURVE_1.X)))\n",
    "ax.plot(CURVE_2.X, CURVE_2.Y, CURVE_2.Z, 'r-', label='Smoothed Curve')\n",
    "pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-4af71d640ac5>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-4af71d640ac5>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    fig.update_traces(marker=dict(size=1))ss\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import plotly\n",
    "from plotly.offline import plot, iplot\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "\n",
    "# For Archea data that looks messy\n",
    "# value_counts = LINKED.PARTICLE.value_counts()\n",
    "# values = value_counts[value_counts.values < 2000]\n",
    "# idx = LINKED.PARTICLE.isin(values.index)\n",
    "# CURVE = LINKED.iloc[idx.values]\n",
    "\n",
    "# CURVE = LINKED[LINKED.PARTICLE == 4]\n",
    "# CURVE = LINKED[(LINKED.PARTICLE != -1)]\n",
    "# CURVE = pd.DataFrame(smoothed_curves, columns=['X', 'Y', 'Z', 'PARTICLE'])\n",
    "# CURVE = CURVE[CURVE.PARTICLE != -1]\n",
    "# CURVE = LINKED[LINKED.PARTICLE == 28]\n",
    "CURVE = smoothed_curves_df[smoothed_curves_df.PARTICLE != -1]\n",
    "# CURVE = smoothed_curves_df[smoothed_curves_df.PARTICLE == 28]\n",
    "\n",
    "# fig = px.scatter_3d(CURVE, x='X', y='Y', z='Z', color='PARTICLE')\n",
    "fig = px.line_3d(CURVE, x='X', y='Y', z='Z', color='PARTICLE', labels='PARTICLE')\n",
    "fig.update_traces(marker=dict(size=1))\n",
    "#plot(fig)\n",
    "iplot(fig)\n",
    "\n",
    "# fig.write_html(PATH[:-3]+'html')\n",
    "# fig.write_html(PATH[:-4]+'_HA.html')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import plotly\n",
    "# from plotly.offline import iplot\n",
    "# plotly.offline.init_notebook_mode()\n",
    "\n",
    "# x = CURVE['X']\n",
    "# y = CURVE['Y']\n",
    "# z = CURVE['Z']\n",
    "# particle = CURVE['PARTICLE']\n",
    "\n",
    "# trace = []\n",
    "# for i in pd.unique(CURVE.PARTICLE):\n",
    "#     dat = CURVE[CURVE['PARTICLE'] == i]\n",
    "#     x = dat['X']\n",
    "#     y = dat['Y']\n",
    "#     z = dat['Z']\n",
    "#     particle = dat['PARTICLE']\n",
    "\n",
    "#     trace1 = go.Scatter3d(\n",
    "#         x=x,\n",
    "#         y=y,\n",
    "#         z=z,\n",
    "#         mode='markers+lines',\n",
    "#         marker=dict(\n",
    "#             size=1,\n",
    "#             color=i,\n",
    "#             colorscale='Viridis',\n",
    "#             opacity=0.8),\n",
    "#         hoverinfo='all',\n",
    "#         )\n",
    "    \n",
    "#     trace.append(trace1)\n",
    "\n",
    "\n",
    "# data = trace\n",
    "# layout = go.Layout(\n",
    "#     margin=dict(\n",
    "#         l=0,\n",
    "#         r=0,\n",
    "#         b=0,\n",
    "#         t=0))\n",
    "\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "# #plot(fig)\n",
    "# iplot(fig, filename='3d-pubg-plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37664bitanaconda3virtualenv2dd5cbc386034cf982a18c7c973e5f38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

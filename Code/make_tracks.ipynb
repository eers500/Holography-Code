{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib as mpl\n",
    "# import mpld3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "mpl.rc('figure',  figsize=(10, 6))\n",
    "import functions as f\n",
    "import sklearn.cluster as cl\n",
    "import time\n",
    "import easygui as gui\n",
    "import hdbscan\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "# from hungarian_algorithm import algorithm\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH = gui.fileopenbox(default='/media/erick/NuevoVol/LINUX_LAP/PhD/')\n",
    "# DF = pd.read_csv(PATH, index_col=1)\n",
    "DF = pd.read_csv(PATH)\n",
    "DF = DF[['X','Y','Z','I_FS','I_GS','FRAME', 'TIME']]\n",
    "# DF = DF.head(1000000)\n",
    "DF.index = np.arange(len(DF))\n",
    "# D = DF[['X', 'Y', 'Z', 'FRAME']].values\n",
    "D = DF.values\n",
    "# DD = A[:, :3]\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Run track detection\n",
    "# KMeans, DBSCAN or HA\n",
    "# method = 'DBSCAN'\n",
    "method = gui.choicebox(msg='Choose a tracking Algorithm', title='Choose', choices=['KMeans', 'DBSCAN']) #choices=['KMeans', 'DBSCAN', 'HA', 'HDBSCAN'])\n",
    "\n",
    "\n",
    "if method == 'KMeans':\n",
    "\n",
    "    #% K-Means for track detection\n",
    "    T0_kmeans = time.time()\n",
    "    kmeans = cl.KMeans(n_clusters=D[D[:, 5] == 0].shape[0], init='k-means++').fit(DF[['X', 'Y', 'Z']])\n",
    "    DF['PARTICLE'] = kmeans.labels_\n",
    "    LINKED = DF\n",
    "    T_kmeans = time.time() -  T0_kmeans\n",
    "    print('T_kmeans', T_kmeans)\n",
    "\n",
    "elif method == 'DBSCAN':\n",
    "    \n",
    "    cores = os.cpu_count()\n",
    "    #% Density Based Spatial Clustering (DBSC)\n",
    "    eps, min_samples = gui.multenterbox(msg='DBSCAN parameters',\n",
    "                            title='DBSCAN parameters',\n",
    "                            fields=['EPSILON (e.g. 5):',\n",
    "                                    'MIN SAMPLES (e.g. 8):']) \n",
    "    # time.sleep(10)\n",
    "    T0_DBSCAN = time.time()\n",
    "    DBSCAN = cl.DBSCAN(eps=float(eps), min_samples=int(min_samples), n_jobs=cores).fit(DF[['X', 'Y', 'Z']])\n",
    "    DF['PARTICLE'] = DBSCAN.labels_\n",
    "    LINKED = DF\n",
    "    L = LINKED.drop(np.where(LINKED.PARTICLE.values == -1)[0])\n",
    "    T_DBSCAN = time.time() - T0_DBSCAN\n",
    "    print('T_DBSCAN', T_DBSCAN)\n",
    "    \n",
    "elif method == 'HDBSCAN':\n",
    "    \n",
    "    cores = os.cpu_count()\n",
    "    #% Density Based Spatial Clustering (DBSC)\n",
    "    # eps, min_samples = gui.multenterbox(msg='DBSCAN parameters',\n",
    "    #                         title='DBSCAN parameters',\n",
    "    #                         fields=['EPSILON (e.g. 5):',\n",
    "                                    # 'MIN SAMPLES (e.g. 8):']) \n",
    "    # time.sleep(10)\n",
    "    T0_HDBSCAN = time.time()\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=30, cluster_selection_epsilon=5, core_dist_n_jobs=cores,  algorithm='boruvka_kdtree')\n",
    "    DF['PARTICLE'] = clusterer.fit_predict(DF[['X','Y','Z']])\n",
    "    LINKED = DF.copy()\n",
    "    L = LINKED.drop(np.where(LINKED.PARTICLE.values == -1)[0])\n",
    "    T_HDBSCAN = time.time() - T0_HDBSCAN\n",
    "    print('T_HDBSCAN', T_HDBSCAN)\n",
    "    \n",
    "\n",
    "elif method == 'HA':\n",
    "\n",
    "    #% Hungrian algorithm\n",
    "    T0_HA = time.time()\n",
    "    FRAME_DATA = np.empty(int(DF['FRAME'].max()), dtype=object)\n",
    "    SIZE = np.empty(FRAME_DATA.shape[0])\n",
    "    for i in range(FRAME_DATA.shape[0]):\n",
    "        FRAME_DATA[i] = D[D[:, 5] == i]\n",
    "        SIZE[i] = FRAME_DATA[i].shape[0]\n",
    "        \n",
    "    # To make a square cost matrix\n",
    "    # SIZES = np.empty(FRAME_DATA.shape[0])\n",
    "    # for i in range(FRAME_DATA.shape[0]):\n",
    "    #     if SIZE.max() != FRAME_DATA[i].shape[0]:\n",
    "    #         DIFF = int(SIZE.max() - FRAME_DATA[i].shape[0])\n",
    "    #         FRAME_DATA[i] = np.pad(FRAME_DATA[i], ((0, DIFF), (0, 0)), 'constant', constant_values=0)\n",
    "    #     SIZES[i] = FRAME_DATA[i].shape[0]\n",
    "            \n",
    "    \n",
    "    # particle_index = np.empty((int(SIZES[0]), len(FRAME_DATA)), dtype='int')\n",
    "    TRACKS = [FRAME_DATA[0]]\n",
    "    sizes = np.empty(len(FRAME_DATA), dtype='int')\n",
    "    sizes[0] = len(FRAME_DATA[0])\n",
    "    for k in range(len(FRAME_DATA)-1):  \n",
    "        print(k)\n",
    "        \n",
    "        # if k==0:    \n",
    "        #     R1 = FRAME_DATA[k]\n",
    "        #     R2 = FRAME_DATA[k+1]\n",
    "        # else:\n",
    "        #     R1 = TRACKS[k]\n",
    "        #     R2 = FRAME_DATA[k+1]\n",
    "        \n",
    "        R1 = FRAME_DATA[k]\n",
    "        R2 = FRAME_DATA[k+1]\n",
    "        \n",
    "        # COST = np.empty((int(SIZE.max()), int(SIZE.max())))\n",
    "        # COST = np.empty((len(R1), len(R2)))\n",
    "        # for i in range(int(SIZE.max())):\n",
    "        #     for j in range(int(SIZE.max())):     \n",
    "      #%\n",
    "        x2, x1 = np.meshgrid(R2[:, 0], R1[:, 0])\n",
    "        y2, y1 = np.meshgrid(R2[:, 1], R1[:, 1])\n",
    "        z2, z1 = np.meshgrid(R2[:, 2], R1[:, 2])        \n",
    "        COST = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\n",
    "                     \n",
    "        # for i in range(len(R1)):\n",
    "        #     for j in range(len(R2)):\n",
    "                \n",
    "        #         COST[i, j] = np.sqrt(sum((R1[i, :3]- R2[j, :3])**2))      \n",
    "        # row_ind, particle_index[:, k] = linear_sum_assignment(COST)\n",
    "        row_ind, particle_index = linear_sum_assignment(COST)\n",
    "        sizes[k+1] = len(particle_index)\n",
    "        # print(len(particle_index))\n",
    "        # TRACKS.append(R2[particle_index[:, k], :])\n",
    "        TRACKS.append(R2[particle_index, :])\n",
    "    \n",
    "    # print(time.time() - T0)\n",
    "    \n",
    "    #%\n",
    "    TRACK = np.vstack(TRACKS)\n",
    "    TR = pd.DataFrame(TRACK, columns=['X', 'Y', 'Z', 'I_FS', 'I_GS', 'FRAME', 'TIME'])\n",
    "    # TR['PARTICLE'] = np.tile(np.arange(SIZES[0]), len(TRACKS))\n",
    "    \n",
    "    particle_column = []\n",
    "    for i in range(len(sizes)):\n",
    "        particle_column.append(np.arange(sizes[i]))    \n",
    "    \n",
    "    particle_column = np.concatenate(particle_column, axis=0)\n",
    "    TR['PARTICLE'] = particle_column\n",
    "    LINKED = TR\n",
    "    T_HA = time.time() - T0_HA\n",
    "    print(T_HA)\n",
    "\n",
    "LINKED = LINKED[LINKED.PARTICLE != -1]\n",
    "LINKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MSD\n",
    "particle_num = np.unique(LINKED['PARTICLE'])\n",
    "\n",
    "pp = []\n",
    "for p in particle_num:\n",
    "    \n",
    "    L = LINKED[LINKED['PARTICLE'] ==  p]\n",
    "    if len(L) > 50:\n",
    "        temp = f.clean_tracks(L)\n",
    "        L = pd.DataFrame.transpose(pd.DataFrame(temp, ['X', 'Y', 'Z', 'TIME', 'FRAME','PARTICLE']))\n",
    "       \n",
    "        _, swim = f.MSD(L.X.values, L.Y.values, L.Z.values)\n",
    "        print(swim)\n",
    "         \n",
    "        if swim == False:\n",
    "            LINKED = LINKED[LINKED['PARTICLE'] != p]\n",
    "      \n",
    "    else:\n",
    "        LINKED = LINKED[LINKED['PARTICLE'] != p]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MSD\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#from matplotlib import pyplot\n",
    "#import functions as f\n",
    "\n",
    "particle_num = np.unique(LINKED['PARTICLE'])\n",
    "\n",
    "LIN = []\n",
    "for p in particle_num:\n",
    "    \n",
    "    L = LINKED[LINKED['PARTICLE'] ==  p]\n",
    "    if len(L) > 50:\n",
    "        # temp = f.clean_tracks(L)\n",
    "        \n",
    "        if len(set(L.FRAME)) != len(L):\n",
    "            temp = f.clean_tracks(L)\n",
    "            #temp = f.clean_tracks_search_sphere(L, 15)\n",
    "            L = pd.DataFrame.transpose(pd.DataFrame(temp, ['X', 'Y', 'Z', 'TIME', 'FRAME','PARTICLE']))\n",
    "            # L = L.drop_duplicates(subset='TIME', keep='last')\n",
    "       \n",
    "            #fig = plt.figure(1)\n",
    "            #ax1 = fig.add_subplot(111, projection='3d') \n",
    "            #ax1.scatter(L['Y'], L['X'], L['Z'], c=L['TIME'])\n",
    "            #pyplot.show()\n",
    "           \n",
    "       \n",
    "        _, swim = f.MSD(L.X.values, L.Y.values, L.Z.values)\n",
    "        #print('swim? = '+str(swim))\n",
    "         \n",
    "        if swim == False:\n",
    "            LINKED = LINKED[LINKED['PARTICLE'] != p]\n",
    "            \n",
    "        elif swim == True:\n",
    "            LIN.append(L)\n",
    "      \n",
    "    else:\n",
    "        LINKED = LINKED[LINKED['PARTICLE'] != p]\n",
    " \n",
    "LINKED = pd.concat(LIN)  \n",
    "LINKED = LINKED.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Smooth trajectories\n",
    "spline_degree = 3  # 3 for cubic spline\n",
    "particle_num = np.sort(LINKED.PARTICLE.unique())\n",
    "T0_smooth = time.time()\n",
    "smoothed_curves = -np.ones((1, 5))\n",
    "for pn in tqdm(particle_num):\n",
    "    # Do not use this\n",
    "    # L = LINKED[LINKED.PARTICLE == pn].values\n",
    "    # X = f.smooth_curve(L, spline_degree=spline_degree, lim=20, sc=3000)\n",
    "    \n",
    "    L = LINKED[LINKED.PARTICLE == pn]\n",
    "    temp = f.clean_tracks(L)\n",
    "    L = pd.DataFrame.transpose(pd.DataFrame(temp, ['X', 'Y', 'Z', 'TIME', 'FRAME','PARTICLE']))\n",
    "\n",
    "    if len(L) < 100:\n",
    "        continue\n",
    "    X = f.csaps_smoothing(L, smoothing_condition=0.999, filter_data=True, limit=1)\n",
    "    \n",
    "    if X != -1:\n",
    "        smoothed_curves = np.vstack((smoothed_curves, np.stack((X[0], X[1], X[2], X[3], pn*np.ones_like(X[1])), axis=1))) \n",
    "\n",
    "smoothed_curves = smoothed_curves[1:, :]\n",
    "smoothed_curves_df = pd.DataFrame(smoothed_curves, columns=['X', 'Y' ,'Z', 'TIME','PARTICLE'])\n",
    "T_smooth = time.time() - T0_smooth\n",
    "\n",
    "# smoothed_curves_df.to_csv(PATH[:-4]+'_DBSCAN_smooth_200.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Crete Data Frame with Speed\n",
    "from matplotlib import pyplot\n",
    "\n",
    "particle_num = np.unique(smoothed_curves_df['PARTICLE'])\n",
    "\n",
    "xx, yy, zz, tt, pp, sp = -1, -1, -1, -1, -1, -1\n",
    "\n",
    "for pn in particle_num:\n",
    "    s = smoothed_curves_df[smoothed_curves_df['PARTICLE'] == pn]\n",
    "    # print(pn, len(s))\n",
    "\n",
    "    if len(s) > 100:\n",
    "        speed, x, y, z, t = f.get_speed(s)\n",
    "        xx = np.hstack((xx, x))\n",
    "        yy = np.hstack((yy, y))\n",
    "        zz = np.hstack((zz, z))\n",
    "        tt = np.hstack((tt, t))\n",
    "        pp = np.hstack((pp, pn*np.ones(len(t))))\n",
    "        sp = np.hstack((sp, speed))\n",
    "    \n",
    "\n",
    "tracks_w_speed = pd.DataFrame(np.transpose([xx[1:], yy[1:], zz[1:], tt[1:], pp[1:], sp[1:]]), columns=['X', 'Y', 'Z', 'TIME', 'PARTICLE', 'SPEED'])\n",
    "\n",
    "# PATH = gui.fileopenbox(default='/media/erick/NuevoVol/LINUX_LAP/PhD/', filetypes='.csv')\n",
    "# tracks_w_speed = pd.read_csv(PATH, index_col=False)\n",
    "\n",
    "fig = plt.figure(1, dpi=150)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "# p = ax.scatter(tracks_w_speed['Y'], tracks_w_speed['X'], tracks_w_speed['Z'], c=tracks_w_speed['SPEED'], marker='.', s=20)\n",
    "# cbar = plt.colorbar(p)\n",
    "# cbar.set_label('Speed ($\\mu ms^{-1}$)')\n",
    "\n",
    "for pn in particle_num:\n",
    "    # s = smoothed_curves_df[smoothed_curves_df['PARTICLE'] == pn]\n",
    "    s = tracks_w_speed[tracks_w_speed['PARTICLE'] == pn]\n",
    "    ax.plot(s['X'], s['Y'], s['Z'], linewidth=2)\n",
    "    # ax.scatter(s['Y'], s['X'], s['Z'])\n",
    "\n",
    "ax.axis('tight')\n",
    "ax.set_title('$\\it{Escherichia \\ Coli}$', fontsize=40)  # $\\it{Escherichia \\ Coli}$\n",
    "ax.set_xlabel('y ($\\mu$m)', fontsize=20)\n",
    "ax.set_ylabel('x ($\\mu$m)', fontsize=20)\n",
    "ax.set_zlabel('-z ($\\mu$m)', fontsize=20)\n",
    "# ax.set_zlim(bottom=0, top=40)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.hist(tracks_w_speed['SPEED'], 13)\n",
    "mean_speed = tracks_w_speed['SPEED'].mean()\n",
    "print(mean_speed)\n",
    "plt.title('Speed: $\\mu$ = ' + str(np.float16(mean_speed)) + ' $\\mu m s^{-1}$', fontsize=40)\n",
    "plt.xlabel('Speed ($\\mu m s^{-1}$)', fontsize=20)\n",
    "plt.ylabel('Frequency', fontsize=20)\n",
    "\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
